{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Week6_03_SENet.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"J24ICquXk1AV","executionInfo":{"status":"ok","timestamp":1634025770659,"user_tz":-540,"elapsed":26457,"user":{"displayName":"이동혁","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07314889758459321342"}}},"source":["# ref: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n","\n","from torch import nn\n","\n","\n","class SELayer(nn.Module):\n","    def __init__(self, channel, reduction=16):\n","        super(SELayer, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channel // reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfcgaIuek1AZ","outputId":"f3aad276-4217-4066-ee9e-dc60a6607eaa"},"source":["# ref: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","# ref: https://hoya012.github.io/blog/DenseNet-Tutorial-2/\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# load dataset\n","trainset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=True,\n","                                        download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=False,\n","                                       download=True, transform=transform)\n","\n","# set transform\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","# train/valid split\n","validation_ratio= 0.1\n","random_seed = 10\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(validation_ratio * num_train))\n","\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","#set loader\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n","                                          sampler=train_sampler, num_workers=2)\n","validloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n","                                          sampler=valid_sampler, num_workers=2)\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.se1 = SELayer(6, 1)\n","\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.se2 = SELayer(16, 2)\n","\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x= self.se1(x)\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.se2(x)\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","net = Net().to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)\n","\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"5Xcnop-ek1Ab","outputId":"6633bfd2-4c07-4112-9a14-4496f7b75d5f"},"source":["best_valid_acc=0\n","for epoch in range(50):  # loop over the dataset multiple times\n","    \n","    net.train()\n","    ### learnint rate scheduling ###\n","    exp_lr_scheduler.step()\n","    \n","    running_loss = 0\n","    total = 0\n","    correct = 0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","\n","        # for train acc\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","    \n","\n","    ### train loss/acc ###\n","    print('[%d] loss: %.3f, train_acc: %.3f %%, lr:%f' %\n","          (epoch + 1, running_loss / 50000, 100.*correct/total, exp_lr_scheduler.get_lr()[0]))\n","    \n","    net.eval()\n","    ### vlaid acc ###\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in validloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    \n","    # save early stopping point (best valid accuracy)\n","    valid_acc= 100.*correct/total\n","    if best_valid_acc<valid_acc:\n","        best_valid_acc= valid_acc\n","        torch.save(net.state_dict(), \"./save_best.pth\") #save\n","    print('[%d] valid_acc: %.3f %%, best: %.3f %%' %(epoch + 1, valid_acc, best_valid_acc))\n","    \n","    \n","\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1] loss: 0.012, train_acc: 37.204 %, lr:0.010000\n","[1] valid_acc: 46.340 %, best: 46.340 %\n","[2] loss: 0.010, train_acc: 48.942 %, lr:0.010000\n","[2] valid_acc: 52.040 %, best: 52.040 %\n","[3] loss: 0.009, train_acc: 52.673 %, lr:0.010000\n","[3] valid_acc: 51.700 %, best: 52.040 %\n","[4] loss: 0.009, train_acc: 54.467 %, lr:0.010000\n","[4] valid_acc: 53.960 %, best: 53.960 %\n","[5] loss: 0.009, train_acc: 55.991 %, lr:0.010000\n","[5] valid_acc: 53.080 %, best: 53.960 %\n","[6] loss: 0.008, train_acc: 57.236 %, lr:0.010000\n","[6] valid_acc: 55.740 %, best: 55.740 %\n","[7] loss: 0.008, train_acc: 58.127 %, lr:0.010000\n","[7] valid_acc: 56.160 %, best: 56.160 %\n","[8] loss: 0.008, train_acc: 58.707 %, lr:0.010000\n","[8] valid_acc: 57.520 %, best: 57.520 %\n","[9] loss: 0.008, train_acc: 59.347 %, lr:0.010000\n","[9] valid_acc: 57.980 %, best: 57.980 %\n","[10] loss: 0.008, train_acc: 60.087 %, lr:0.010000\n","[10] valid_acc: 55.520 %, best: 57.980 %\n","[11] loss: 0.008, train_acc: 60.700 %, lr:0.010000\n","[11] valid_acc: 58.480 %, best: 58.480 %\n","[12] loss: 0.008, train_acc: 60.911 %, lr:0.010000\n","[12] valid_acc: 56.560 %, best: 58.480 %\n","[13] loss: 0.008, train_acc: 61.393 %, lr:0.010000\n","[13] valid_acc: 57.520 %, best: 58.480 %\n","[14] loss: 0.008, train_acc: 61.527 %, lr:0.010000\n","[14] valid_acc: 58.760 %, best: 58.760 %\n","[15] loss: 0.008, train_acc: 61.889 %, lr:0.010000\n","[15] valid_acc: 57.940 %, best: 58.760 %\n","[16] loss: 0.007, train_acc: 62.438 %, lr:0.010000\n","[16] valid_acc: 60.240 %, best: 60.240 %\n","[17] loss: 0.007, train_acc: 63.344 %, lr:0.010000\n","[17] valid_acc: 59.760 %, best: 60.240 %\n","[18] loss: 0.007, train_acc: 63.636 %, lr:0.010000\n","[18] valid_acc: 59.440 %, best: 60.240 %\n","[19] loss: 0.007, train_acc: 63.693 %, lr:0.010000\n","[19] valid_acc: 59.540 %, best: 60.240 %\n","[20] loss: 0.006, train_acc: 68.958 %, lr:0.001000\n","[20] valid_acc: 62.860 %, best: 62.860 %\n","[21] loss: 0.006, train_acc: 70.116 %, lr:0.001000\n","[21] valid_acc: 62.760 %, best: 62.860 %\n","[22] loss: 0.006, train_acc: 70.727 %, lr:0.001000\n","[22] valid_acc: 63.180 %, best: 63.180 %\n","[23] loss: 0.006, train_acc: 71.113 %, lr:0.001000\n","[23] valid_acc: 63.000 %, best: 63.180 %\n","[24] loss: 0.006, train_acc: 71.484 %, lr:0.001000\n","[24] valid_acc: 62.980 %, best: 63.180 %\n","[25] loss: 0.006, train_acc: 71.838 %, lr:0.001000\n","[25] valid_acc: 63.000 %, best: 63.180 %\n","[26] loss: 0.006, train_acc: 71.842 %, lr:0.001000\n","[26] valid_acc: 62.640 %, best: 63.180 %\n","[27] loss: 0.005, train_acc: 72.342 %, lr:0.001000\n","[27] valid_acc: 63.000 %, best: 63.180 %\n","[28] loss: 0.005, train_acc: 72.620 %, lr:0.001000\n","[28] valid_acc: 62.700 %, best: 63.180 %\n","[29] loss: 0.005, train_acc: 72.776 %, lr:0.001000\n","[29] valid_acc: 62.420 %, best: 63.180 %\n","[30] loss: 0.005, train_acc: 72.889 %, lr:0.001000\n","[30] valid_acc: 62.660 %, best: 63.180 %\n","[31] loss: 0.005, train_acc: 73.176 %, lr:0.001000\n","[31] valid_acc: 62.460 %, best: 63.180 %\n","[32] loss: 0.005, train_acc: 73.329 %, lr:0.001000\n","[32] valid_acc: 62.560 %, best: 63.180 %\n","[33] loss: 0.005, train_acc: 73.571 %, lr:0.001000\n","[33] valid_acc: 62.220 %, best: 63.180 %\n","[34] loss: 0.005, train_acc: 73.767 %, lr:0.001000\n","[34] valid_acc: 62.080 %, best: 63.180 %\n","[35] loss: 0.005, train_acc: 74.031 %, lr:0.001000\n","[35] valid_acc: 61.880 %, best: 63.180 %\n","[36] loss: 0.005, train_acc: 74.136 %, lr:0.001000\n","[36] valid_acc: 62.180 %, best: 63.180 %\n","[37] loss: 0.005, train_acc: 74.242 %, lr:0.001000\n","[37] valid_acc: 62.420 %, best: 63.180 %\n","[38] loss: 0.005, train_acc: 74.316 %, lr:0.001000\n","[38] valid_acc: 61.920 %, best: 63.180 %\n","[39] loss: 0.005, train_acc: 74.627 %, lr:0.001000\n","[39] valid_acc: 62.020 %, best: 63.180 %\n","[40] loss: 0.005, train_acc: 76.238 %, lr:0.000100\n","[40] valid_acc: 62.100 %, best: 63.180 %\n","[41] loss: 0.005, train_acc: 76.336 %, lr:0.000100\n","[41] valid_acc: 62.140 %, best: 63.180 %\n","[42] loss: 0.005, train_acc: 76.453 %, lr:0.000100\n","[42] valid_acc: 62.080 %, best: 63.180 %\n","[43] loss: 0.005, train_acc: 76.482 %, lr:0.000100\n","[43] valid_acc: 62.160 %, best: 63.180 %\n","[44] loss: 0.005, train_acc: 76.516 %, lr:0.000100\n","[44] valid_acc: 62.100 %, best: 63.180 %\n","[45] loss: 0.005, train_acc: 76.569 %, lr:0.000100\n","[45] valid_acc: 61.880 %, best: 63.180 %\n","[46] loss: 0.005, train_acc: 76.593 %, lr:0.000100\n","[46] valid_acc: 62.100 %, best: 63.180 %\n","[47] loss: 0.005, train_acc: 76.673 %, lr:0.000100\n","[47] valid_acc: 62.020 %, best: 63.180 %\n","[48] loss: 0.005, train_acc: 76.713 %, lr:0.000100\n","[48] valid_acc: 62.180 %, best: 63.180 %\n","[49] loss: 0.005, train_acc: 76.749 %, lr:0.000100\n","[49] valid_acc: 62.160 %, best: 63.180 %\n","[50] loss: 0.005, train_acc: 76.827 %, lr:0.000100\n","[50] valid_acc: 62.300 %, best: 63.180 %\n","Finished Training\n"]}]},{"cell_type":"code","metadata":{"id":"FuAEJovek1Ac","outputId":"6842b571-830c-4b03-dbb6-367a089be7c2"},"source":["# load & eval\n","\n","load_model = Net().to(device)\n","load_model.load_state_dict(torch.load(\"./save_best.pth\"))\n","\n","load_model.eval()\n","\n","### test acc ###\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        outputs = load_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_acc= 100.*correct/total\n","\n","print('test_acc: %.3f %%' %(test_acc))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["test_acc: 62.440 %\n"]}]},{"cell_type":"code","metadata":{"id":"tx4xbJv-k1Ac"},"source":[""],"execution_count":null,"outputs":[]}]}