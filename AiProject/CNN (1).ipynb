{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"XxhfEMQk_vLI"},"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGNa5m-K_xOr"},"source":["seed = 42\n","\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owTuMH3LBm0J"},"source":["#이미지 파일 이름을 이용해서, frame 숫자를 뽑아내는 함수이다.\n","def Find_frame_num(image_filename):\n","  frame_num = ''\n","  count_ = 0\n","  \n","\n","  for i in range(len(image_filename)):\n","    if image_filename[i] == '_':\n","      count_ += 1\n","      if count_ == 2:\n","        break\n","      continue\n","    if count_ == 1:\n","      frame_num += image_filename[i]\n","  \n","  frame_num = int(frame_num)\n","  return frame_num\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tis69giBjVh"},"source":["#yolo용 target 구조: (x, y, w, h, c, cat, dog, i, j)\n","#만약 CNN 학습을 위해 target에 breed를 추가하신다면, (breed, x, y, w, h, c, cat, dog, i, j)\n","\n","#Image에 필요한 target을 만들려고함\n","#근데 존나 마음아픈게 frame별로 걍 마구잡이로 되 있어서, Image파일 이름에 있는 frame 숫자를 이용해서 멥핑. /ARCH/20201028_cat-arch-000156.mp4/frame_102_timestamp_0.jpg => 102  \n","#이 함수는 라벨(json)과 frame_num을 인자로 받아 해당 frame에 대한 bounding box, speices, responsible grid cell을 찾는다. *추가: 만약 breed가 추가된다면 breed도...\n","def Make_target(label_dir, frame_num):\n","  with open(label_dir) as f:\n","    label_json = json.load(f)\n","  \n","  s = 8\n","\n","  target_list = []\n","  #바운딩박스 찾기\n","  target_list.append(label_json['metadata']['breed'])\n","  for i in range(len(label_json['annotations'])):\n","    if label_json['annotations'][i]['frame_number'] == frame_num:\n","       target_list.append(label_json['annotations'][i]['bounding_box']['x'])\n","       target_list.append(label_json['annotations'][i]['bounding_box']['y'])\n","       target_list.append(label_json['annotations'][i]['bounding_box']['width'])\n","       target_list.append(label_json['annotations'][i]['bounding_box']['height'])\n","  \n","  #바운딩박스 yolo의 target 형식에 맞게 변경\n","  image_width = label_json['metadata']['width']\n","  image_height = label_json['metadata']['height']\n","\n","  s_image_width = image_width / 8\n","  s_image_height = image_height / 8\n","\n","  i, j = 0, 0\n","  while(i*s_image_width < target_list[0]):\n","    i += 1\n","\n","  while(j*s_image_height < target_list[1]):\n","    j += 1\n","\n","  i -= 1\n","  j -= 1\n","  \n","  target_list[0] = target_list[0]/s_image_width - i\n","  target_list[1] = target_list[1]/s_image_height - j\n","  target_list[2] /= s_image_width\n","  target_list[3] /= s_image_height\n","\n","\n","  #confidence score를 1로 설정\n","  target_list.append(1.0)\n","\n","  #고양이면 1, 0 을 강아지면 0,1을 append\n","  if label_json['metadata']['species'] == 'CAT':\n","    target_list.append(1.0)\n","    target_list.append(0)\n","  else:\n","    target_list.append(0)\n","    target_list.append(1.0)\n","\n","  target_list.append(i)\n","  target_list.append(j)\n","  \n","  return target_list  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gTv8GEm9JqJo"},"source":["#Label파일과 image파일에 약간의? 문제가 있는 관계로 고민을 좀 많이했다.\n","#그 결과, dataset class getitem에서 image랑 그 image에 맞는 필요한 부분만 뽑아서 사용하려 한다. \n","\n","class cnn_dataset(Dataset):\n","  def __init__(self, label_dir, image_dir, transform=None):\n","    self.label_dir = label_dir\n","    self.image_dir = image_dir\n","    self.transform = transform\n","\n","    list_image = os.listdir(self.image_dir)\n","    self.list_image = list_image\n","\n","  #이미지 하나 당 target 생성\n","  def __getitem__(self, index):\n","    image = os.path.join(self.image_dir, self.list_image[index])\n","    image = Image.open(image).convert('RGB')\n","    #image = np.asarray(image)\n","\n","    target = Make_target(self.label_dir, Find_frame_num(self.list_image[index])) #밑에 Make_target함수와 Find_frame_num함수 설명이 있습니다.\n","    \n","    if self.transform:\n","      image = self.transform(image)\n","\n","    target = torch.tensor(target)\n","\n","    data = {'image':image, 'target':target}\n","    return data\n","\n","  def __len__(self):\n","    return len(self.list_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDxI2e9PCWdR"},"source":["cnn_trans = transforms.Compose([transforms.Resize((512,512)),\n","                                transforms.ToTensor()])\n","\n","trainset = cnn_dataset()\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpGWVyJR_zSa"},"source":["size = 32\n","\n","class BreedNet(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","    \n","        self.conv1 = nn.Conv2d(3, size, kernel_size=3, stride=1, padding = 1)\n","        self.conv2 = nn.Conv2d(size, size*2, kernel_size=3, stride=1, padding = 1)\n","        self.conv3 = nn.Conv2d(size*2, size*4, kernel_size=3, stride=1, padding = 1)\n","                \n","        self.fc1 = nn.Linear(128*28*28, 512)\n","        self.fc2 = nn.Linear(512, 128)\n","        \n","        self.dropout = nn.Dropout(0.5) \n","        \n","    def forward(self, x):\n","        x = self.MaxPool2d(F.relu(self.conv1(x)))\n","        x = self.MaxPool2d(F.relu(self.conv2(x)))\n","        x = self.MaxPool2d(F.relu(self.conv3(x)))\n","\n","        x = x.view(-1, size*4*28*28)\n","        x = self.dropout(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzAgMunhCKRz"},"source":["epochs = 5\n","\n","model_cat = BreedNet().to(device)\n","model_dog = BreedNet().to(device)\n","optimizer_cat = optim.Adadelta(model_cat.parameters(), lr=1.0)\n","scheduler_cat = StepLR(optimizer_cat, step_size=1, gamma=0.7)\n","optimizer_dog = optim.Adadelta(model_dog.parameters(), lr=1.0)\n","scheduler_dog = StepLR(optimizer_dog, step_size=1, gamma=0.7)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","model_cat.train()\n","model_dog.train()\n","\n","for epoch in range(EPOCHS):\n","    for i, (image, target) in enumerate(train_loader):\n","        image, target = image.to(device), target.to(device)\n","        optimizer.zero_grad()\n","\n","        if target[6] == 1.0:\n","            out = model_cat(image)\n","            loss = criterion(out, targets)\n","            loss.backward()\n","            train_loss += loss.item()\n","            optimizer_cat.step()\n","\n","            if (i + 1) % 100 == 0:\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, EPOCHS, i + 1, total_step, loss.item()))\n","        else:\n","            out = model_dog(image)\n","            loss = criterion(out, targets)\n","            loss.backward()\n","            train_loss += loss.item()\n","            optimizer_cat.step()\n","\n","            if (i + 1) % 100 == 0:\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, EPOCHS, i + 1, total_step, loss.item()))\n","      scheduler_cat.step()\n","      scheduler_dog.step()"],"execution_count":null,"outputs":[]}]}